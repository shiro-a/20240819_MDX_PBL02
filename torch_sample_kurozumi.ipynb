{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯PyTorchç‰ˆã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "å…¬å¼ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¯2ï½3å¹´å‰ã«ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã€åˆ©ç”¨æ•°ãŒæ¸›ã£ã¦ãã¦ã„ã‚‹TensorFlowãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚  \n",
    "ç¾åœ¨ã¯PyTorchãŒä¸»æµã§ã‚ã‚‹ãŸã‚ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å…¬å¼ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¨åŒæ§˜ã®å‡¦ç†ã‚’PyTorchç‰ˆã«æ›¸ãæ›ãˆãŸç‰©ã§ã™ã€‚  \n",
    "ã¾ãŸã€åŠ¹ç‡çš„ã«é–‹ç™ºã™ã‚‹ãŸã‚ã€PyTorchã¨ã‚»ãƒƒãƒˆã§åˆ©ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„PyTorch Lightningã‚‚ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€Google Colabï¼ˆ2024å¹´8æœˆ21æ—¥æ™‚ç‚¹ï¼‰ã‚„ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®Dockerç’°å¢ƒã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚\n",
    "\n",
    "ã‚³ãƒ¼ãƒ‰ä½œæˆï¼š PBL02ã€ŒğŸ¯ã¨ã‚‰ã€ãƒãƒ¼ãƒ  CM_Kurozumi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ç’°å¢ƒã®äº‹å‰æº–å‚™\n",
    "\n",
    "Google Colab ã§å‹•ã‹ã—ã¦ã„ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®å‰æº–å‚™å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "Google Drive ã¸ã®æ¥ç¶šç¢ºèªç”»é¢ãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€æŒ‡ç¤ºã«å¾“ã£ã¦æ¥ç¶šã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "- å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆPyTorch Lightning ã¨ TorchMetricsï¼‰\n",
    "- Google Drive ã¸ã®æ¥ç¶š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Google Colab ä¸Šã§å®Ÿè¡Œã—ã¦ã„ã‚‹ã‹ã©ã†ã‹\n",
    "ON_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# fmt: off\n",
    "if ON_COLAB:\n",
    "    print(\"Running on Google Colab.\")\n",
    "    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    !pip install lightning==2.4.0 torchmetrics==1.4.1\n",
    "\n",
    "    # Google Drive ã«ãƒã‚¦ãƒ³ãƒˆ\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running on Google Colab.\")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡¦ç†ã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚  \n",
    "ä¸»ã«PyTorchç³»ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸­å¿ƒã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã®æŒ‡å®š\n",
    "\n",
    "åˆ©ç”¨ç’°å¢ƒã«åˆã‚ã›ã¦ã€å­¦ç¿’ãªã©ã«åˆ©ç”¨ã™ã‚‹ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ `DATA_DIR` ã¨ã€  \n",
    "å­¦ç¿’æ™‚ã®ãƒ­ã‚°ã‚„çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€ `LOGS_DIR` ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "Google Colab ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆã¯ã€Google Drive ã« `DXQuest` ã¨ã„ã†ãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã€  \n",
    "ãã®ä¸­ã« `train`, `test` ãªã©ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹æƒ³å®šã«ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒ‘ã‚¹æƒ…å ±ã¯ã€åˆ©ç”¨ã—ã¦ã„ã‚‹ç’°å¢ƒã«åˆã‚ã›ã¦é©å®œæ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ©ç”¨ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€åï¼ˆColabã‹ã©ã†ã‹ã§ãƒ‘ã‚¹ã‚’å¤‰æ›´ï¼‰\n",
    "DATA_DIR = \"/content/drive/MyDrive/DXQuest\" if ON_COLAB else \"/workspace/data\"\n",
    "LOGS_DIR = \"/content/drive/MyDrive/logs\" if ON_COLAB else \"/workspace/logs\"\n",
    "\n",
    "print(\"Data folder:\", DATA_DIR)\n",
    "print(\"Logs folder:\", LOGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "\n",
    "å­¦ç¿’æ™‚ã®ã€Œãƒãƒƒãƒã‚µã‚¤ã‚ºã€ã‚„ã€ã‚¯ãƒ©ã‚¹æ•°ã®è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "ã¾ãŸã€å†…éƒ¨ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ä¹±æ•°ã®ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®šã—ã¦ã€å†å®Ÿè¡Œæ™‚ã®å†ç¾æ€§ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚  \n",
    "ä¹±æ•°å€¤ã®ã‚·ãƒ¼ãƒ‰ã¯ `42` ã‚’æŒ‡å®šã—ã¦ã„ã¾ã™ãŒã€ã“ã®å€¤ã«å¤§ããªæ„å‘³ã¯ç„¡ã„ã®ã§ã€å¥½ããªæ•°å­—ã«ã—ã¦OKã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# å„ç¨®ä¹±æ•°ã®å›ºå®š\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–å­¦ç¿’ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®šç¾©\n",
    "\n",
    "å­¦ç¿’ã¨æ¤œè¨¼ã«åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹å‡¦ç†ã§ã™ã€‚  \n",
    "å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«ã€ãã‚Œãã‚Œä»¥ä¸‹ã®å‡¦ç†ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "\n",
    "- å®Ÿéš›ã«åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®é›†ã¾ã‚Šï¼ˆdatasetï¼‰\n",
    "- å„ç”»åƒã«å¯¾ã™ã‚‹åŠ å·¥å‡¦ç†ï¼ˆtransformï¼‰\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™å‡¦ç†ï¼ˆdata_loaderï¼‰\n",
    "\n",
    "ğŸ’¡å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿\n",
    "\n",
    "- æœ¬æ¥ã¯ã€å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€Œåˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã€ã‚’åˆ©ç”¨ã—ã¾ã™\n",
    "- ãŸã ã—ã€ã“ã“ã§ã¯å…¬å¼ã‚µãƒ³ãƒ—ãƒ«ã¨åŒæ§˜ã« `train` ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ã§ã‚‚åˆ©ç”¨ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(DATA_DIR, \"train\")  # æ¤œè¨¼ã«ã‚‚trainãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ï¼ˆæœ¬æ¥ã¯åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã¹ãï¼‰\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å¤‰æ›\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # ResNetç”¨ã«ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "WORKERS = 2\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆãƒ©ãƒ™ãƒ«ã¯ãƒ•ã‚©ãƒ«ãƒ€åã‹ã‚‰è‡ªå‹•ã§è¨­å®šã•ã‚Œã‚‹ï¼‰\n",
    "train_dataset = ImageFolder(train_dir, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "# Note:ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã¯validationãƒ‡ãƒ¼ã‚¿ã‚‚trainãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã£ã¦ã„ã‚‹ãŒã€æœ¬æ¥ã¯åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã¹ãï¼\n",
    "val_dataset = ImageFolder(val_dir, transform=data_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageFolder` ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã¨ã€ãƒ•ã‚©ãƒ«ãƒ€åã§è‡ªå‹•ã§æ­£è§£ãƒ©ãƒ™ãƒ«ãŒè¨­å®šã•ã‚Œã¾ã™ã€‚  \n",
    "å®Ÿéš›ã«è¨­å®šã•ã‚ŒãŸã€Œæ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã€ãã®å€¤ã€ã¯ã€datasetã® `class_to_idx` ã§ç¢ºèªã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå‹•ã§è¨­å®šã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ç•ªå·ã¨ãƒ©ãƒ™ãƒ«åã®ç¢ºèª\n",
    "print(train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ãƒ¢ãƒ‡ãƒ«ãªã©ã®å®šç¾©\n",
    "\n",
    "å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ãªã©ã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "ã“ã“ã§ã¯ã€åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã‚„æ§‹é€ ã€æå¤±é–¢æ•°ã€æœ€é©åŒ–æ‰‹æ³•ãªã©ã‚’å®šç¾©ã—ã¾ã™ã€‚  \n",
    "ã¾ãŸã€å­¦ç¿’ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¡Œã†å‡¦ç†ã‚‚å®šç¾©ã§ãã¾ã™ã€‚\n",
    "\n",
    "ä»Šå›ã¯ResNet18ã‚’åˆ©ç”¨ã—ã¦ã€4ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "\n",
    "Accuracy, Precision, Recall, F1-Scoreãªã©ã®è¨ˆç®—ã‚‚ã€torchmetricsã‚’ä½¿ã£ã¦ç°¡å˜ã«è¨ˆç®—ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«å®šç¾©\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "        self.recall = Recall(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        self.log(\"train_acc\", self.accuracy(outputs, labels), on_epoch=True)\n",
    "        self.log(\"train_precision\", self.precision(outputs, labels), on_epoch=True)\n",
    "        self.log(\"train_recall\", self.recall(outputs, labels), on_epoch=True)\n",
    "        self.log(\"train_f1\", self.f1(outputs, labels), on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.accuracy(outputs, labels), on_epoch=True)\n",
    "        self.log(\"val_precision\", self.precision(outputs, labels), on_epoch=True)\n",
    "        self.log(\"val_recall\", self.recall(outputs, labels), on_epoch=True)\n",
    "        self.log(\"val_f1\", self.f1(outputs, labels), on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # æœ€é©åŒ–é–¢æ•°ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šï¼ˆæ€§èƒ½ãŒå‘ä¸Šã—ãªããªã£ãŸã‚‰å­¦ç¿’ç‡ã‚’æ¸›ã‚‰ã™ï¼‰\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        images, _ = batch\n",
    "        outputs = self(images)\n",
    "        return torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "# å„ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®çµæœã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "class PrintCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        print(\n",
    "            f\" [{trainer.current_epoch:03}] \"\n",
    "            f\"loss: {metrics['train_loss']:.4f}, \"\n",
    "            f\"acc: {metrics['train_acc']:.4f}, \"\n",
    "            f\"precision: {metrics['train_precision']:.4f}, \"\n",
    "            f\"recall: {metrics['train_recall']:.4f}, \"\n",
    "            f\"f1: {metrics['train_f1']:.4f}, \"\n",
    "            f\"val_loss: {metrics['val_loss']:.4f}, \"\n",
    "            f\"val_acc: {metrics['val_acc']:.4f}, \"\n",
    "            f\"val_precision: {metrics['val_precision']:.4f}, \"\n",
    "            f\"val_recall: {metrics['val_recall']:.4f}, \"\n",
    "            f\"val_f1: {metrics['val_f1']:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–å­¦ç¿’è¨ˆç”»ã®æº–å‚™\n",
    "\n",
    "PyTorch Lightning ã‚’ä½¿ã£ã¦ã€å­¦ç¿’ã®æº–å‚™ã‚’è¡Œã„ã¾ã™ã€‚  \n",
    "å…¨ä½“çš„ãªå­¦ç¿’ã®ç®¡ç†ã‚’è¡Œã†ã‚ˆã†ãªã€Œãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã€ã®è¨­å®šã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "`MAX_EPOCHS` ã®å€¤ã‚’å¤‰æ›´ã™ã‚Œã°ã€å­¦ç¿’ã•ã›ã‚‹æœ€å¤§å›æ•°ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ï¼‰ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚  \n",
    "ã¾ãŸã€å­¦ç¿’ãŒé€²ã¾ãªããªã£ãŸå ´åˆã¯ã€ãã“ã§å­¦ç¿’ã‚’ã‚¹ãƒˆãƒƒãƒ—ã•ã›ã‚‹EarlyStoppingã‚‚è¨­å®šã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ğŸ’¡EarlyStoppingã«é–¢ã™ã‚‹æ³¨æ„ç‚¹\n",
    "ã“ã“ã§ã¯ `val_loss`ï¼ˆæ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹çµæœï¼‰ãŒã€3å›é€£ç¶šã§æ”¹å–„ã•ã‚Œãªã„å ´åˆã¯ã‚¹ãƒˆãƒƒãƒ—ã•ã›ã‚‹è¨­å®šã«ã—ã¦ã„ã¾ã™ã€‚  \n",
    "ãŸã ã—ã€æœ¬ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã¯ã€æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚‚ã€Œå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ã†ã€å½¢ã«ã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã®çŠ¶æ…‹ã§ã¯ã€Œval_lossã«å¯¾ã™ã‚‹çµæœã¯å¸¸ã«è‰¯ããªã‚Šç¶šã‘ã‚‹ï¼ˆãã—ã¦éå­¦ç¿’ãŒèµ·ãã‚‹ï¼‰ã€ãŸã‚ã€EarlyStoppingãŒç™ºå‹•ã™ã‚‹ã“ã¨ã¯ç„¡ã„ã¨æ€ã‚ã‚Œã¾ã™ã€‚  \n",
    "EarlyStoppingã¯ã€å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ãã¡ã‚“ã¨åˆ†ã‘ãŸå ´åˆã«åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 10\n",
    "\n",
    "# å­¦ç¿’æ™‚ã®ãƒ­ã‚°è¨˜éŒ²ï¼ˆå¿…è¦ã«å¿œã˜ã¦W&Bãªã©ã®ãƒ­ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆ©ç”¨å¯èƒ½ï¼‰\n",
    "logger = None\n",
    "\n",
    "# EarlyStoppingã®è¨­å®š\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "# Trainerã®è¨­å®š\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=LOGS_DIR,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[early_stopping, PrintCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–å­¦ç¿’\n",
    "\n",
    "ä¸Šè¨˜ã§å®šç¾©ã—ãŸãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒã€è‡ªå‹•ã§å­¦ç¿’ã‚’é€²ã‚ã¦ã„ãã¾ã™ã€‚  \n",
    "å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿é‡ã‚„ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã€ç’°å¢ƒãªã©ã«ã‚ˆã£ã¦ã€å­¦ç¿’æ™‚é–“ã¯å¤§ããç•°ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "å­¦ç¿’ã«ã¯æ•°åˆ†ï½æ•°æ™‚é–“ç¨‹åº¦ã®æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã¯ `LOGS_DIR` ã§æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®ã€  \n",
    "`lightning_logs/version_n/checkpoints` ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor CoreãŒä½¿ãˆã‚‹å ´åˆã¯åˆ©ç”¨ã™ã‚‹\n",
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "model = ResNetClassifier()\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚  \n",
    "å­¦ç¿’ã™ã‚‹åº¦ã«ã€å­¦ç¿’æ¸ˆã¿ã®ã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ*.ckptï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ¸ˆã¿ã®ã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "def enum_checkpoints(dir_path: str = LOGS_DIR) -> list:\n",
    "    checkpoint_files = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".ckpt\"):\n",
    "                checkpoint_files.append(os.path.join(root, file))\n",
    "    return sorted(checkpoint_files)\n",
    "\n",
    "\n",
    "checkpoints = enum_checkpoints()\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¨˜ã§å–å¾—ã—ãŸã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‹ã‚‰ã€Œæœ€æ–°ã®ã‚¦ã‚§ã‚¤ãƒˆæƒ…å ±ã€ã‚’é¸ã‚“ã§èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚  \n",
    "æœ€å¾Œã®ã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ€ã‚‚ç²¾åº¦ãŒé«˜ã„ã¨ã¯é™ã‚‰ãªã„ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦èª­ã¿è¾¼ã‚€ã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUãŒä½¿ãˆã‚‹å ´åˆã¯GPUã‚’ä½¿ã†ï¼ˆGPUãŒãªã„å ´åˆã¯CPUã‚’ä½¿ã†ï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# å­¦ç¿’æ¸ˆã¿ã®ã‚¦ã‚§ã‚¤ãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã‚€\n",
    "ckpt_file = checkpoints[-1]  # æœ€æ–°ã®ã‚¦ã‚§ã‚¤ãƒˆã‚’èª­ã¿è¾¼ã‚€ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "print(\"Checkpoint:\", ckpt_file)\n",
    "checkpoint = torch.load(ckpt_file)\n",
    "model = ResNetClassifier()\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "testç”»åƒã«å¯¾ã—ã¦æ¨è«–ã—ã¦ã„ããŸã‚ã€ãƒ†ã‚¹ãƒˆç”»åƒç”¨ã® dataset, data_loader ã‚’å®šç¾©ã—ã¾ã™ã€‚  \n",
    "ç”»åƒã‚’åŠ å·¥ã™ã‚‹å‡¦ç†ã¯ã€trainãªã©ã§ä½œæˆã—ãŸ transform ã‚’ãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "testãƒ‡ãƒ¼ã‚¿ã«ã¯æ­£è§£ãƒ©ãƒ™ãƒ«ãŒç„¡ã„ï¼ˆregularãªã©ã®æ­£è§£ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„ï¼‰ãŸã‚ã€  \n",
    "å°‚ç”¨ã® `UnlabeledImageDataset` ã‚’å®šç¾©ã—ã¾ã™ã€‚ï¼ˆImageFolderã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€ãŒç„¡ã„ã¨åˆ©ç”¨ã§ããªã„ãŸã‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageLoaderã¯ãƒ©ãƒ™ãƒ«ä»˜ãã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã€ãƒ©ãƒ™ãƒ«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "# /workspace/data/test ã‹ã‚‰å®Ÿéš›ã«æ¨è«–ã™ã‚‹ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "test_dataset = UnlabeledImageDataset(test_dir, transform=data_transforms)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–ãƒ†ã‚¹ãƒˆç”»åƒã«å¯¾ã—ã¦æ¨è«–\n",
    "\n",
    "èª­ã¿è¾¼ã‚“ã ãƒ†ã‚¹ãƒˆç”»åƒã«å¯¾ã—ã¦ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’è¡Œã£ã¦ã„ãã¾ã™ã€‚  \n",
    "æ¨è«–çµæœã¯ã€å„ç”»åƒã«å¯¾ã—ã¦ 0ï½3 ã®å­¦ç¿’æ™‚ã«ä½¿ç”¨ã—ãŸæ­£è§£ãƒ©ãƒ™ãƒ«ã®ç•ªå·ï¼ˆ0=bridge ãªã©ï¼‰ã§è¿”ã£ã¦ãã¾ã™ã€‚\n",
    "\n",
    "å…¨ã¦ã®ç”»åƒã«å¯¾ã™ã‚‹æ¨è«–çµæœã‚’ã€ãƒªã‚¹ãƒˆå½¢å¼ã«æˆå½¢ã—ã¦å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ¨è«–\n",
    "all_preds = []\n",
    "for i, images in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–æ¨è«–çµæœã‚’DataFrameã§æ•´ç†\n",
    "\n",
    "æœ€çµ‚çš„ã«ã¯ã€Œãƒ•ã‚¡ã‚¤ãƒ«åã¨ã€è‰¯å“=0/ä¸è‰¯å“=1ã€ã®å€¤ã‚’ã‚»ãƒƒãƒˆã§å‡ºåŠ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€DataFrameå‹ã§è¡¨å½¢å¼ã§æ•´ç†ã—ã¾ã™ã€‚\n",
    "\n",
    "åˆ†ã‹ã‚Šã‚„ã™ã„ã‚ˆã†ã«ã€äºˆæ¸¬çµæœã®æ•°å€¤ï¼ˆ0ï½3ï¼‰ã«å¯¾ã™ã‚‹ãƒ©ãƒ™ãƒ«ã‚„ã€  \n",
    "æœ€çµ‚çš„ãªã€Œregular=0ï¼ˆè‰¯å“ï¼‰ã€ãã‚Œä»¥å¤–ã¯1ï¼ˆä¸è‰¯å“ï¼‰ã€ã¨ã„ã†å½¢ã¸ã®å¤‰æ›ã‚‚ã€ã“ã“ã§è¡Œã„ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ã¨ã€æ¨è«–çµæœã‚’DataFrameã«ã¾ã¨ã‚ã‚‹\n",
    "df = pd.DataFrame({\"path\": test_dataset.image_paths, \"pred\": all_preds})\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã€ãƒ•ã‚¡ã‚¤ãƒ«åã€æå‡ºç”¨ã®äºˆæ¸¬çµæœï¼ˆè‰¯å“=0/ä¸è‰¯å“=1ï¼‰ã‚’è¿½åŠ \n",
    "df[\"label\"] = df[\"pred\"].apply(lambda x: train_dataset.classes[x])  # ãƒ©ãƒ™ãƒ«ï¼ˆä¾‹: \"regular\"ï¼‰\n",
    "df[\"filename\"] = df[\"path\"].apply(lambda x: os.path.basename(x))  # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: \"003.jpeg\"ï¼‰\n",
    "df[\"y_hat\"] = df[\"label\"].apply(lambda x: 0 if x == \"regular\" else 1)  # è‰¯å“=0/ä¸è‰¯å“=1\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«åã®æ˜‡é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æŒ¯ã‚Šç›´ã—\n",
    "df = df.sort_values(\"filename\").reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”–æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§TSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "\n",
    "æœ€çµ‚çš„ã«æå‡ºã™ã‚‹TSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚  \n",
    "ä¸­èº«ã‚‚ã€Œãƒ•ã‚¡ã‚¤ãƒ«åã¨0/1ã€ã®å€¤ã®ã‚»ãƒƒãƒˆã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§å‡ºåŠ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ã“ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "TSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ `DATA_DIR` ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã«ã€ my_submission_{æ—¥æ™‚}.tsv ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«åã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚  \n",
    "ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€SIGNATE Cloud çµŒç”±ã§æå‡ºã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®è¨­å®šï¼ˆç¾åœ¨æ—¥æ™‚ã‚’ä»˜ã‘ã¦ãŠãï¼‰\n",
    "now_str = pd.Timestamp.now(tz=\"Asia/Tokyo\").strftime(\"%Y%m%d_%H%M%S\")\n",
    "tsv_filename = os.path.join(DATA_DIR, f\"my_submission_{now_str}.tsv\")\n",
    "\n",
    "# filename, y_hat ã®é …ç›®ã ã‘ã‚’tsvãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ˜ãƒƒãƒ€ç„¡ã—ï¼‰\n",
    "df[[\"filename\", \"y_hat\"]].to_csv(tsv_filename, sep=\"\\t\", header=False, index=False)\n",
    "print(\"Saved:\", tsv_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
